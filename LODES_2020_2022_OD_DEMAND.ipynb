{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c50f9cb-60a9-4b83-af25-70f2d4eb12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60a61635-9e62-4ed2-93ac-d01ea6cffaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration ready.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "STATE = \"ny\"\n",
    "YEARS = [2020, 2021, 2022]\n",
    "JOB_TYPE = \"JT00\"  # all jobs\n",
    "BASE_URL = f\"https://lehd.ces.census.gov/data/lodes/LODES8/{STATE}/od/{STATE}_od_main_{JOB_TYPE}_{{year}}.csv.gz\"\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "IMAGES_DIR = \"images\"\n",
    "OD_MATRIX_FILE_SUM = \"OD_demand_LODES.csv\"\n",
    "TOP_N = 15\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e54137a8-2af2-49a1-8b8d-af195fe9d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing NYC county and tract mappings...\n",
      "NYC county filters and tract mappings initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP COUNTY FILTERS & MAPPINGS ---\n",
    "print(\"\\nPreparing NYC county and tract mappings...\")\n",
    "\n",
    "nyc_county_fips = ['36005', '36047', '36061', '36081', '36085']\n",
    "county_to_borough_digit = {\n",
    "    '36061': '1',  # Manhattan\n",
    "    '36005': '2',  # Bronx\n",
    "    '36047': '3',  # Brooklyn\n",
    "    '36081': '4',  # Queens\n",
    "    '36085': '5'   # Staten Island\n",
    "}\n",
    "\n",
    "print(\"NYC county filters and tract mappings initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66756a47-758d-4d31-863c-f41c754565fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and aggregating LODES data (low-memory mode)...\n",
      "\n",
      "Loading 2020 from https://lehd.ces.census.gov/data/lodes/LODES8/ny/od/ny_od_main_JT00_2020.csv.gz\n",
      "   File size: 0.038 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Processing 2020: 8chunk [00:42,  5.33s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Finished 2020: 980,500 OD pairs loaded.\n",
      "   Time: 56.8 sec\n",
      "\n",
      "Loading 2021 from https://lehd.ces.census.gov/data/lodes/LODES8/ny/od/ny_od_main_JT00_2021.csv.gz\n",
      "   File size: 0.039 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Processing 2021: 8chunk [00:38,  4.77s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Finished 2021: 985,215 OD pairs loaded.\n",
      "   Time: 53.1 sec\n",
      "\n",
      "Loading 2022 from https://lehd.ces.census.gov/data/lodes/LODES8/ny/od/ny_od_main_JT00_2022.csv.gz\n",
      "   File size: 0.041 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Processing 2022: 8chunk [00:50,  6.34s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Finished 2022: 1,027,581 OD pairs loaded.\n",
      "   Time: 65.7 sec\n",
      "\n",
      "===============================================================\n",
      "TOTAL DATA DOWNLOADED: 0.118 GB\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD AND PROCESS ONE YEAR AT A TIME ---\n",
    "print(\"\\nLoading and aggregating LODES data (low-memory mode)...\")\n",
    "\n",
    "# Track total size\n",
    "total_bytes_downloaded = 0\n",
    "\n",
    "# Initialize empty aggregation dataframe\n",
    "od_matrix_total = pd.DataFrame(columns=[\"origin_tract\", \"destination_tract\", \"S000\"])\n",
    "\n",
    "def get_file_size(url):\n",
    "    \"\"\"HEAD request to get accurate compressed file size.\"\"\"\n",
    "    try:\n",
    "        r = requests.head(url, timeout=10)\n",
    "        return int(r.headers.get(\"Content-Length\", 0))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for y in YEARS:\n",
    "    url = BASE_URL.format(year=y)\n",
    "    print(f\"\\nLoading {y} from {url}\")\n",
    "\n",
    "    # --- Get File Size ---\n",
    "    size_bytes = get_file_size(url)\n",
    "    if size_bytes:\n",
    "        total_bytes_downloaded += size_bytes\n",
    "        print(f\"   File size: {size_bytes / (1024**3):.3f} GB\")\n",
    "    else:\n",
    "        print(\"   File size: Unknown\")\n",
    "\n",
    "    # Timing\n",
    "    t0 = time.time()\n",
    "\n",
    "    try:\n",
    "        # Load compressed CSV in chunks to avoid memory spikes\n",
    "        chunks = pd.read_csv(url, dtype=str, chunksize=1_000_000)\n",
    "\n",
    "        year_sum = {}\n",
    "\n",
    "        # Wrap chunks inside tqdm\n",
    "        for chunk in tqdm(chunks, desc=f\"   Processing {y}\", unit=\"chunk\"):\n",
    "\n",
    "            # Filter to NYC boroughs\n",
    "            chunk = chunk[\n",
    "                chunk['h_geocode'].astype(str).str[:5].isin(nyc_county_fips) &\n",
    "                chunk['w_geocode'].astype(str).str[:5].isin(nyc_county_fips)\n",
    "            ].copy()\n",
    "\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            # Convert job count\n",
    "            chunk['S000'] = (\n",
    "                pd.to_numeric(chunk['S000'], errors='coerce')\n",
    "                .fillna(0)\n",
    "                .astype('int32')\n",
    "            )\n",
    "\n",
    "            # Build custom tract IDs\n",
    "            chunk['origin_tract'] = (\n",
    "                chunk['h_geocode'].astype(str).str[:5].map(county_to_borough_digit)\n",
    "                + chunk['h_geocode'].astype(str).str[5:11]\n",
    "            )\n",
    "            chunk['destination_tract'] = (\n",
    "                chunk['w_geocode'].astype(str).str[:5].map(county_to_borough_digit)\n",
    "                + chunk['w_geocode'].astype(str).str[5:11]\n",
    "            )\n",
    "\n",
    "            # Aggregate within chunk\n",
    "            agg = chunk.groupby(\n",
    "                ['origin_tract', 'destination_tract']\n",
    "            )['S000'].sum()\n",
    "\n",
    "            # Merge chunk sums into running dictionary\n",
    "            for (o, d), val in agg.items():\n",
    "                key = (o, d)\n",
    "                year_sum[key] = year_sum.get(key, 0) + val\n",
    "\n",
    "        # Append yearly totals into master DataFrame\n",
    "        df_year = pd.DataFrame(\n",
    "            [(o, d, v) for (o, d), v in year_sum.items()],\n",
    "            columns=[\"origin_tract\", \"destination_tract\", \"S000\"]\n",
    "        )\n",
    "        od_matrix_total = pd.concat([od_matrix_total, df_year], ignore_index=True)\n",
    "\n",
    "        print(f\"   Finished {y}: {len(df_year):,} OD pairs loaded.\")\n",
    "        print(f\"   Time: {time.time() - t0:.1f} sec\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipping {y} due to error: {e}\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n===============================================================\")\n",
    "print(f\"TOTAL DATA DOWNLOADED: {total_bytes_downloaded / (1024**3):.3f} GB\")\n",
    "print(\"===============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1c85362-b723-4bf9-9766-ea19cd5933ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summing across all available years (2020–2022)...\n",
      "Combined OD matrix ready: 1,804,941 total OD pairs.\n"
     ]
    }
   ],
   "source": [
    "# --- COMBINE AND SUM ACROSS ALL YEARS ---\n",
    "print(\"\\nSumming across all available years (2020–2022)...\")\n",
    "\n",
    "od_matrix_sum = (\n",
    "    od_matrix_total.groupby(['origin_tract', 'destination_tract'])['S000']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'S000': 'total_commuters_2020_2022'})\n",
    ")\n",
    "\n",
    "print(f\"Combined OD matrix ready: {len(od_matrix_sum):,} total OD pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80f936bb-1aed-4915-b211-ce39d7ebebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saving output to CSV...\n",
      "Saved aggregated OD matrix to: output/OD_demand_LODES.csv\n",
      "\n",
      "Preview:\n",
      "  origin_tract destination_tract total_commuters_2020_2022\n",
      "0      1000100           1000700                         3\n",
      "1      1000100           1000900                         5\n",
      "2      1000100           1001501                         1\n",
      "3      1000100           1001502                         3\n",
      "4      1000100           1001600                         1\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE FINAL OUTPUT ---\n",
    "print(\"\\n Saving output to CSV...\")\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, OD_MATRIX_FILE_SUM)\n",
    "od_matrix_sum.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved aggregated OD matrix to: {output_path}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(od_matrix_sum.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f3b85a8-1abb-4fed-9278-ed3dbc8a0ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering OD pairs (drop self and neighbors)\n",
      "Loaded OD matrix: 1,804,941 rows\n",
      "Loaded 2,163 neighbor lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering OD pairs: 100%|█████████████████████████████████████████████████████████████████████████████| 1804941/1804941 [00:02<00:00, 664644.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed self-pairs: 2,249\n",
      "Removed neighbor pairs: 14,873\n",
      "Kept 1,787,819 of 1,804,941 OD rows\n",
      "Saved filtered & sorted file to: output/OD_demand_LODES_nonneighbors.csv\n",
      "Same-tract pairs remaining: 0\n",
      "\n",
      "--- Top 10 OD pairs AFTER filtering (sorted by commuters) ---\n",
      "        origin_tract destination_tract  total_commuters_2020_2022\n",
      "185982       1025500           1020300                        756\n",
      "96763        1014700           1008000                        695\n",
      "77641        1012400           1025100                        609\n",
      "34788        1005200           1010100                        604\n",
      "184982       1025300           1020300                        544\n",
      "1701512      5004003           5014606                        530\n",
      "181838       1024500           1020300                        497\n",
      "868724       3053102           3053300                        488\n",
      "1128832      3123700           3053300                        484\n",
      "31140        1004400           1010900                        444\n"
     ]
    }
   ],
   "source": [
    "# --- Filter OD pairs to removeneighbors and self-pairs, then sort output ---\n",
    "\n",
    "print(\"\\nFiltering OD pairs (drop self and neighbors)\")\n",
    "\n",
    "# Paths\n",
    "od_path = os.path.join(OUTPUT_DIR, \"OD_demand_LODES.csv\")\n",
    "neighbors_csv_path = os.path.join(OUTPUT_DIR, \"nyc_tract_neighbors_1mile.csv\")\n",
    "filtered_path = os.path.join(OUTPUT_DIR, \"OD_demand_LODES_nonneighbors.csv\")\n",
    "\n",
    "# --- Load OD matrix and normalize IDs ---\n",
    "df_od = pd.read_csv(od_path, dtype=str)\n",
    "for col in (\"origin_tract\", \"destination_tract\"):\n",
    "    df_od[col] = df_od[col].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(7)\n",
    "\n",
    "print(f\"Loaded OD matrix: {len(df_od):,} rows\")\n",
    "\n",
    "# --- Load neighbors into dictionary of sets ---\n",
    "neighbors_dict = {}\n",
    "with open(neighbors_csv_path, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        tid = str(row[\"tract_id\"]).zfill(7)\n",
    "        nlist = [n.strip().zfill(7) for n in row[\"neighbor_ids\"].split(\",\") if n.strip()]\n",
    "        neighbors_dict[tid] = set(nlist)\n",
    "\n",
    "print(f\"Loaded {len(neighbors_dict):,} neighbor lists\")\n",
    "\n",
    "# --- Filter out OD pairs that are (a) self-pairs, or (b) within 1 mile ---\n",
    "mask = []\n",
    "removed_self = removed_neighbor = 0\n",
    "\n",
    "for o, d in tqdm(zip(df_od[\"origin_tract\"], df_od[\"destination_tract\"]),\n",
    "                 total=len(df_od), desc=\"Filtering OD pairs\"):\n",
    "    if o == d:\n",
    "        removed_self += 1\n",
    "        mask.append(False)\n",
    "    elif d in neighbors_dict.get(o, set()):\n",
    "        removed_neighbor += 1\n",
    "        mask.append(False)\n",
    "    else:\n",
    "        mask.append(True)\n",
    "\n",
    "df_filtered = df_od[mask].reset_index(drop=True)\n",
    "print(f\"Removed self-pairs: {removed_self:,}\")\n",
    "print(f\"Removed neighbor pairs: {removed_neighbor:,}\")\n",
    "print(f\"Kept {len(df_filtered):,} of {len(df_od):,} OD rows\")\n",
    "\n",
    "# --- Convert commuter counts to numeric and sort descending ---\n",
    "df_filtered[\"total_commuters_2020_2022\"] = pd.to_numeric(\n",
    "    df_filtered[\"total_commuters_2020_2022\"], errors=\"coerce\"\n",
    ")\n",
    "df_filtered = df_filtered.sort_values(\"total_commuters_2020_2022\", ascending=False)\n",
    "\n",
    "# --- Save filtered and sorted results ---\n",
    "df_filtered.to_csv(filtered_path, index=False)\n",
    "print(f\"Saved filtered & sorted file to: {filtered_path}\")\n",
    "\n",
    "# --- Sanity check ---\n",
    "same_pairs = (df_filtered[\"origin_tract\"] == df_filtered[\"destination_tract\"]).sum()\n",
    "print(f\"Same-tract pairs remaining: {same_pairs}\")\n",
    "\n",
    "print(\"\\n--- Top 10 OD pairs AFTER filtering (sorted by commuters) ---\")\n",
    "print(df_filtered.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59e00f2a-4af8-4ff9-b8fb-0c06dcf79e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing how much commuter volume was removed...\n",
      "Total commuters before: 8,832,130\n",
      "Total commuters after:  8,257,840\n",
      "Removed commuters:      574,290 (6.50% of total)\n",
      "\n",
      "--- Top 10 OD pairs BEFORE filtering ---\n",
      "        origin_tract  destination_tract  total_commuters_2020_2022\n",
      "153199       1019900            1020300                       2497\n",
      "161203       1021100            1020300                       2094\n",
      "157867       1020701            1020300                       2063\n",
      "156550       1020500            1020300                       1592\n",
      "38076        1005501            1006500                       1397\n",
      "83671        1012400            1011600                       1342\n",
      "72369        1010602            1011600                       1171\n",
      "82534        1012102            1010100                       1097\n",
      "63789        1009200            1009200                       1086\n",
      "149823       1019500            1020300                        864\n",
      "\n",
      "--- Top 10 OD pairs AFTER filtering ---\n",
      "   origin_tract  destination_tract  total_commuters_2020_2022\n",
      "0       1025500            1020300                        756\n",
      "1       1014700            1008000                        695\n",
      "2       1012400            1025100                        609\n",
      "3       1005200            1010100                        604\n",
      "4       1025300            1020300                        544\n",
      "5       5004003            5014606                        530\n",
      "6       1024500            1020300                        497\n",
      "7       3053102            3053300                        488\n",
      "8       3123700            3053300                        484\n",
      "9       1004400            1010900                        444\n"
     ]
    }
   ],
   "source": [
    "# --- ANALYZE IMPACT OF FILTERING ---\n",
    "print(\"\\nAnalyzing how much commuter volume was removed...\")\n",
    "\n",
    "# Reload both (in case previous df_od and df_filtered aren't in memory)\n",
    "df_od = pd.read_csv(os.path.join(OUTPUT_DIR, \"OD_demand_LODES.csv\"))\n",
    "df_filtered = pd.read_csv(os.path.join(OUTPUT_DIR, \"OD_demand_LODES_nonneighbors.csv\"))\n",
    "\n",
    "# Ensure numeric\n",
    "df_od['total_commuters_2020_2022'] = pd.to_numeric(df_od['total_commuters_2020_2022'], errors='coerce').fillna(0)\n",
    "df_filtered['total_commuters_2020_2022'] = pd.to_numeric(df_filtered['total_commuters_2020_2022'], errors='coerce').fillna(0)\n",
    "\n",
    "# Total commuters before and after\n",
    "total_before = df_od['total_commuters_2020_2022'].sum()\n",
    "total_after = df_filtered['total_commuters_2020_2022'].sum()\n",
    "diff = total_before - total_after\n",
    "pct_diff = (diff / total_before) * 100\n",
    "\n",
    "print(f\"Total commuters before: {total_before:,.0f}\")\n",
    "print(f\"Total commuters after:  {total_after:,.0f}\")\n",
    "print(f\"Removed commuters:      {diff:,.0f} ({pct_diff:.2f}% of total)\")\n",
    "\n",
    "# --- Top 10 OD pairs before and after ---\n",
    "top_before = df_od.sort_values('total_commuters_2020_2022', ascending=False).head(10)\n",
    "top_after = df_filtered.sort_values('total_commuters_2020_2022', ascending=False).head(10)\n",
    "\n",
    "print(\"\\n--- Top 10 OD pairs BEFORE filtering ---\")\n",
    "print(top_before[['origin_tract', 'destination_tract', 'total_commuters_2020_2022']])\n",
    "\n",
    "print(\"\\n--- Top 10 OD pairs AFTER filtering ---\")\n",
    "print(top_after[['origin_tract', 'destination_tract', 'total_commuters_2020_2022']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08ebb0-32be-408f-9702-de3f4cf60900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team10project",
   "language": "python",
   "name": "team10project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
